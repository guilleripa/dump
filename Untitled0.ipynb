{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOg3hN6fKJSo9z2U1i8Bk0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilleripa/dump/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NHsKT-7WHFod"
      },
      "outputs": [],
      "source": [
        "from typing import Generator\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter: pd.DataFrame) -> Generator[str, None, None]:\n",
        "    for _, text, _ in data_iter.itertuples():\n",
        "        yield tokenizer(text)\n",
        "\n",
        "\n",
        "class CustomDataset:\n",
        "    def __init__(self, data: pd.DataFrame):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data.iloc[idx]\n",
        "\n",
        "\n",
        "def get_dataloaders(\n",
        "    train_data: pd.DataFrame, test_data: pd.DataFrame, batch_size: int = 16\n",
        ") -> DataLoader:\n",
        "    vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    train_data[\"tokens\"] = train_data[\"question_text\"].apply(\n",
        "        lambda x: vocab(tokenizer(x))\n",
        "    )\n",
        "    test_data[\"tokens\"] = test_data[\"question_text\"].apply(\n",
        "        lambda x: vocab(tokenizer(x))\n",
        "    )\n",
        "\n",
        "    train_dataset = CustomDataset(train_data)\n",
        "    test_dataset = CustomDataset(test_data)\n",
        "    train_dataset.vocab = vocab\n",
        "    test_dataset.vocab = vocab\n",
        "\n",
        "    def collate_batch(batch):\n",
        "        \"\"\"Collate function to pad the text to the maximum length in a batch.\n",
        "        The batch is sorted in descending order of text length to minimize the\n",
        "        amount of padding needed.\n",
        "        \"\"\"\n",
        "        label_list, text_list, lengths = [], [], []\n",
        "        for _text, _label, _tokens in batch:\n",
        "            label_list.append(_label)\n",
        "            # processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "            processed_text = torch.tensor(_tokens)\n",
        "            text_list.append(processed_text)\n",
        "            if processed_text.size(0) == 0:\n",
        "                lengths.append(torch.tensor(1))\n",
        "                print(\"Empty text found!\", _text, _label, processed_text)\n",
        "            else:\n",
        "                lengths.append(processed_text.size(0))\n",
        "\n",
        "        text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
        "        label_list = torch.tensor(label_list, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "        # sort based on text lengths\n",
        "        lengths = torch.tensor(lengths)\n",
        "        _, perm_idx = lengths.sort(0, descending=True)\n",
        "\n",
        "        text_list = text_list[perm_idx]\n",
        "        label_list = label_list[perm_idx]\n",
        "        lengths = lengths[perm_idx]\n",
        "\n",
        "        return label_list.to(device), text_list.to(device), lengths\n",
        "\n",
        "    train_ddl = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
        "    )\n",
        "    test_ddl = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
        "    )\n",
        "    return train_ddl, test_ddl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Obtener la matriz de embeddings desde el diccionario     ---> FREEZARLO (PARA NO CATASTROFIC FORGETING)\n",
        "# embeddings_matrix = torch.tensor(list(embeddings_index.values()))\n",
        "\n",
        "\n",
        "# Define una clase para el modelo\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        n_layers,\n",
        "        bidirectional,\n",
        "        dropout,\n",
        "    ):\n",
        "        self.bidirectional = bidirectional\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        # Capa de embedding con los embeddings pre-entrenados (la congele)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Capa RNN (LSTM en este caso)\n",
        "        self.rnn = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=self.bidirectional,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        # Capa lineal de salida\n",
        "        self.fc = nn.Linear(\n",
        "            hidden_dim * 2 if self.bidirectional else hidden_dim, output_dim\n",
        "        )\n",
        "\n",
        "        # Capa de dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, text_lengths, batch_first=True\n",
        "        )\n",
        "        _, (hidden, _) = self.rnn(packed_embedded)\n",
        "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
        "        #     packed_output, batch_first=True\n",
        "        # )\n",
        "        hidden = self.dropout(\n",
        "            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "            if self.bidirectional\n",
        "            else hidden[-1, :, :]\n",
        "        )\n",
        "        output = self.fc(hidden)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "SVxYUt6EHI_l"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# import f1 score\n",
        "from sklearn.metrics import f1_score\n",
        "from torch import nn\n",
        "\n",
        "# CURRENT_DIR = Path(__file__).parent\n",
        "CURRENT_DIR = Path(\"/content\")\n",
        "DATA_DIR = CURRENT_DIR\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, epoch):\n",
        "    total_acc, total_count, total_f1_score = 0, 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch_num, (label, text, text_lens) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # Compute prediction and loss\n",
        "        predicted_label = model(text, text_lens)\n",
        "        loss = loss_fn(predicted_label, label)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # clip the gradient norm to 1.0 to prevent exploding gradients. A common\n",
        "        # problem with RNNs and LSTMs is the \"exploding gradient\" problem. This\n",
        "        # is where the gradient for a particular parameter gets larger and larger\n",
        "        # as the number of layers increases. This can result in the gradient\n",
        "        # becoming so large that the weights overflow (i.e. become NaN) and the\n",
        "        # model fails to train.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "\n",
        "        predictions = (predicted_label > 0.5).float()\n",
        "\n",
        "        total_acc += (predictions == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        # total_f1_score += f1_score(\n",
        "        #         label.detach().cpu(), predictions.detach().cpu(), average=\"macro\"\n",
        "        #     )\n",
        "\n",
        "        if batch_num % log_interval == 0 and batch_num > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f}\"\n",
        "                # \"| f1_score {:8.3f}\"\n",
        "                \"| ms/batch {:5.2f}\".format(\n",
        "                    epoch,\n",
        "                    batch_num,\n",
        "                    len(dataloader),\n",
        "                    total_acc / total_count,\n",
        "                    # total_f1_score / log_interval,\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                )\n",
        "            )\n",
        "            total_acc, total_count, total_f1_score = 0, 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(dataloader, model, criterion):\n",
        "    model.eval()\n",
        "    total_acc, total_count, total_f1_score = 0, 0, 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = (model(text, offsets) > 0.5).float()\n",
        "            total_loss += criterion(predicted_label, label)\n",
        "\n",
        "            total_acc += (predicted_label == label).sum().item()\n",
        "            total_f1_score += f1_score(\n",
        "                label.detach().cpu(), predicted_label.detach().cpu(), average=\"macro\"\n",
        "            )\n",
        "            total_count += label.size(0)\n",
        "\n",
        "    print(\n",
        "        \"Evaluation - loss: {:.6f}  \"\n",
        "        \"accuracy: {:.3f}  f1_score: {:.3f}\\n\".format(\n",
        "            total_loss / len(dataloader),\n",
        "            total_acc / total_count,\n",
        "            total_f1_score / len(dataloader),\n",
        "        )\n",
        "    )\n",
        "    return total_acc / total_count, total_f1_score / len(dataloader)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load csv dataset\n",
        "    train_data = pd.read_csv(DATA_DIR / \"train_set.csv\")\n",
        "    train_data = train_data[train_data[\"question_text\"].str.len() > 5]\n",
        "    test_data = pd.read_csv(DATA_DIR / \"test_set.csv\")\n",
        "    test_data = test_data[test_data[\"question_text\"].str.len() > 5]\n",
        "\n",
        "    # Turn csv to dataloader\n",
        "    # train_dataloader = get_dataloader(train_data, batch_size=BATCH_SIZE)\n",
        "    # test_dataloader = get_dataloader(test_data, batch_size=BATCH_SIZE)\n",
        "    train_dataloader, test_dataloader = get_dataloaders(train_data, test_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Instantiate model\n",
        "    model = RNNModel(\n",
        "        vocab_size=len(train_dataloader.dataset.vocab),\n",
        "        embedding_dim=8,\n",
        "        hidden_dim=8,\n",
        "        output_dim=1,\n",
        "        n_layers=1,\n",
        "        bidirectional=True,\n",
        "        dropout=0,\n",
        "    )\n",
        "\n",
        "    # Send model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize the loss function\n",
        "    pos_weight = torch.tensor([7.0])\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    epochs = 10\n",
        "    # Measure time\n",
        "    tic = time.time()\n",
        "    for t in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        print(f\"Epoch {t}\\n-------------------------------\")\n",
        "        train_loop(train_dataloader, model, loss_fn, optimizer, t)\n",
        "        # eval\n",
        "        accu_val, f1_val = evaluate(test_dataloader, model, loss_fn)\n",
        "        print(\"-\" * 59)\n",
        "        print(\n",
        "            \"| end of epoch {:3d} | time: {:5.2f}s\"\n",
        "            \"| valid accuracy {:8.3f} \"\n",
        "            \"| valid f1 score {:8.3f}\".format(\n",
        "                t,\n",
        "                time.time() - epoch_start_time,\n",
        "                accu_val,\n",
        "                f1_val,\n",
        "            )\n",
        "        )\n",
        "    print(\"-\" * 59)\n",
        "    toc = time.time()\n",
        "    print(\"Done!\", f\"Training time: {toc - tic:>.3f} seconds\")\n",
        "    # Train model\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, epochs+1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4T_PI_uDHzSj",
        "outputId": "ab54103a-ea6b-4717-9b8e-fde0ea0fdeff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "-------------------------------\n",
            "| epoch   0 |   500/ 2526 batches | accuracy    0.728| ms/batch 20.30\n",
            "| epoch   0 |  1000/ 2526 batches | accuracy    0.775| ms/batch 17.34\n",
            "| epoch   0 |  1500/ 2526 batches | accuracy    0.801| ms/batch 19.79\n",
            "| epoch   0 |  2000/ 2526 batches | accuracy    0.816| ms/batch 20.31\n",
            "| epoch   0 |  2500/ 2526 batches | accuracy    0.825| ms/batch 18.08\n",
            "Evaluation - loss: 1.166254  accuracy: 0.820  f1_score: 0.764\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   0 | time: 58.38s| valid accuracy    0.820 | valid f1 score    0.764\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "| epoch   1 |   500/ 2526 batches | accuracy    0.841| ms/batch 19.72\n",
            "| epoch   1 |  1000/ 2526 batches | accuracy    0.847| ms/batch 19.80\n",
            "| epoch   1 |  1500/ 2526 batches | accuracy    0.856| ms/batch 16.81\n",
            "| epoch   1 |  2000/ 2526 batches | accuracy    0.860| ms/batch 19.77\n",
            "| epoch   1 |  2500/ 2526 batches | accuracy    0.861| ms/batch 20.33\n",
            "Evaluation - loss: 1.134776  accuracy: 0.870  f1_score: 0.818\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 57.27s| valid accuracy    0.870 | valid f1 score    0.818\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "| epoch   2 |   500/ 2526 batches | accuracy    0.872| ms/batch 19.37\n",
            "| epoch   2 |  1000/ 2526 batches | accuracy    0.873| ms/batch 19.99\n",
            "| epoch   2 |  1500/ 2526 batches | accuracy    0.877| ms/batch 18.25\n",
            "| epoch   2 |  2000/ 2526 batches | accuracy    0.874| ms/batch 18.20\n",
            "| epoch   2 |  2500/ 2526 batches | accuracy    0.878| ms/batch 19.53\n",
            "Evaluation - loss: 1.120932  accuracy: 0.868  f1_score: 0.819\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 58.44s| valid accuracy    0.868 | valid f1 score    0.819\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "| epoch   3 |   500/ 2526 batches | accuracy    0.884| ms/batch 16.92\n",
            "| epoch   3 |  1000/ 2526 batches | accuracy    0.885| ms/batch 19.90\n",
            "| epoch   3 |  1500/ 2526 batches | accuracy    0.885| ms/batch 19.35\n",
            "| epoch   3 |  2000/ 2526 batches | accuracy    0.890| ms/batch 17.47\n",
            "| epoch   3 |  2500/ 2526 batches | accuracy    0.886| ms/batch 19.43\n",
            "Evaluation - loss: 1.116732  accuracy: 0.856  f1_score: 0.809\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 57.15s| valid accuracy    0.856 | valid f1 score    0.809\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "| epoch   4 |   500/ 2526 batches | accuracy    0.894| ms/batch 18.41\n",
            "| epoch   4 |  1000/ 2526 batches | accuracy    0.894| ms/batch 17.74\n",
            "| epoch   4 |  1500/ 2526 batches | accuracy    0.893| ms/batch 19.77\n",
            "| epoch   4 |  2000/ 2526 batches | accuracy    0.893| ms/batch 19.65\n",
            "| epoch   4 |  2500/ 2526 batches | accuracy    0.896| ms/batch 16.62\n",
            "Evaluation - loss: 1.111892  accuracy: 0.877  f1_score: 0.830\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 56.59s| valid accuracy    0.877 | valid f1 score    0.830\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "| epoch   5 |   500/ 2526 batches | accuracy    0.900| ms/batch 20.00\n",
            "| epoch   5 |  1000/ 2526 batches | accuracy    0.901| ms/batch 17.72\n",
            "| epoch   5 |  1500/ 2526 batches | accuracy    0.898| ms/batch 18.91\n",
            "| epoch   5 |  2000/ 2526 batches | accuracy    0.900| ms/batch 19.20\n",
            "| epoch   5 |  2500/ 2526 batches | accuracy    0.898| ms/batch 18.68\n",
            "Evaluation - loss: 1.110276  accuracy: 0.874  f1_score: 0.827\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 56.88s| valid accuracy    0.874 | valid f1 score    0.827\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "| epoch   6 |   500/ 2526 batches | accuracy    0.908| ms/batch 19.81\n",
            "| epoch   6 |  1000/ 2526 batches | accuracy    0.903| ms/batch 19.78\n",
            "| epoch   6 |  1500/ 2526 batches | accuracy    0.903| ms/batch 17.31\n",
            "| epoch   6 |  2000/ 2526 batches | accuracy    0.902| ms/batch 19.38\n",
            "| epoch   6 |  2500/ 2526 batches | accuracy    0.904| ms/batch 19.59\n",
            "Evaluation - loss: 1.116887  accuracy: 0.892  f1_score: 0.843\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 57.71s| valid accuracy    0.892 | valid f1 score    0.843\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "| epoch   7 |   500/ 2526 batches | accuracy    0.910| ms/batch 18.09\n",
            "| epoch   7 |  1000/ 2526 batches | accuracy    0.907| ms/batch 19.66\n",
            "| epoch   7 |  1500/ 2526 batches | accuracy    0.909| ms/batch 19.64\n",
            "| epoch   7 |  2000/ 2526 batches | accuracy    0.908| ms/batch 16.89\n",
            "| epoch   7 |  2500/ 2526 batches | accuracy    0.909| ms/batch 19.37\n",
            "Evaluation - loss: 1.110624  accuracy: 0.866  f1_score: 0.820\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 57.61s| valid accuracy    0.866 | valid f1 score    0.820\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "| epoch   8 |   500/ 2526 batches | accuracy    0.915| ms/batch 17.02\n",
            "| epoch   8 |  1000/ 2526 batches | accuracy    0.912| ms/batch 19.82\n",
            "| epoch   8 |  1500/ 2526 batches | accuracy    0.912| ms/batch 19.46\n",
            "| epoch   8 |  2000/ 2526 batches | accuracy    0.911| ms/batch 18.33\n",
            "| epoch   8 |  2500/ 2526 batches | accuracy    0.911| ms/batch 18.06\n",
            "Evaluation - loss: 1.113992  accuracy: 0.879  f1_score: 0.831\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 57.03s| valid accuracy    0.879 | valid f1 score    0.831\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "| epoch   9 |   500/ 2526 batches | accuracy    0.917| ms/batch 19.39\n",
            "| epoch   9 |  1000/ 2526 batches | accuracy    0.914| ms/batch 16.90\n",
            "| epoch   9 |  1500/ 2526 batches | accuracy    0.916| ms/batch 19.97\n",
            "| epoch   9 |  2000/ 2526 batches | accuracy    0.915| ms/batch 19.38\n",
            "| epoch   9 |  2500/ 2526 batches | accuracy    0.916| ms/batch 17.30\n",
            "Evaluation - loss: 1.119613  accuracy: 0.893  f1_score: 0.844\n",
            "\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 56.95s| valid accuracy    0.893 | valid f1 score    0.844\n",
            "-----------------------------------------------------------\n",
            "Done! Training time: 574.024 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-cacc56cea548>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Training time: {toc - tic:>.3f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: train_loop() missing 2 required positional arguments: 'optimizer' and 'epoch'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ecgtfuxnIHdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}